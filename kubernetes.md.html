<!DOCTYPE html>
<html>
<meta charset="utf-8" />
<title>Kubernetes</title>
<xmp theme="readable" style="display:none;" toc="true">
# 安装部署
## kubeadm
### 安装
所有 node 上执行。
```
apt-get update && apt-get install -y apt-transport-https
curl https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | apt-key add -
cat <<EOF >/etc/apt/sources.list.d/kubernetes.list
deb https://mirrors.aliyun.com/kubernetes/apt/ kubernetes-xenial main
EOF
apt-get update
apt-get install -y kubelet kubeadm kubectl
```

最新稳定版：`https://dl.k8s.io/release/stable-1.txt` (v1.13.2)
镜像：`kubeadm config images list`
```
k8s.gcr.io/kube-apiserver:v1.13.2
k8s.gcr.io/kube-controller-manager:v1.13.2
k8s.gcr.io/kube-scheduler:v1.13.2
k8s.gcr.io/kube-proxy:v1.13.2
k8s.gcr.io/pause:3.1
k8s.gcr.io/etcd:3.2.24
k8s.gcr.io/coredns:1.2.6
```

手动拉镜像并重命名
```
#!/bin/bash
images=`kubeadm config images list`
for i in $images; do
	image=registry.cn-hangzhou.aliyuncs.com/google_containers/${i#k8s.gcr.io/}
	docker pull $image
	docker tag $image $i
done
```

### init master
`kubeadm init --config config.yaml`
默认配置：`kubeadm config print init/join-defaults`
详细说明：https://godoc.org/k8s.io/kubernetes/cmd/kubeadm/app/apis/kubeadm/v1beta1

config.yaml
```
apiVersion: kubeadm.k8s.io/v1beta1
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.212.107
  bindPort: 6443
nodeRegistration:
  kubeletExtraArgs:
    dynamic-config-dir: /var/lib/kubelet/dynamic-config
---
apiVersion: kubeadm.k8s.io/v1beta1
kind: ClusterConfiguration
apiServer:
  extraArgs:
    enable-admission-plugins: NodeRestriction,ExtendedResourceToleration
networking:
  podSubnet: "10.100.0.0/16" # 需与 calico 配置一致，且不与 host 网络冲突
imageRepository: "registry.cn-hangzhou.aliyuncs.com/google_containers"
kubernetesVersion: "v1.13.2"
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
featureGates:
    DynamicKubeletConfig: true
```

网络插件
```
kubectl apply -f https://docs.projectcalico.org/v3.4/getting-started/kubernetes/installation/hosted/kubernetes-datastore/calico-networking/1.7/calico.yaml
```

### join node
`kubeadm token create --print-join-command`
`kubeadm join <ip>:<port> --token <token> --discovery-token-ca-cert-hash <hash>`

### 删除 node
```
kubectl drain <node name> --delete-local-data --force --ignore-daemonsets
kubectl delete node <node name>
kubeadm reset
```

reset iptable:
`iptables -F && iptables -t nat -F && iptables -t mangle -F && iptables -X`

### 更新配置
组件：直接更新 `/etc/kubernetes/manifests` 配置
[kubelet](https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/): 使用[configMap](https://kubernetes.io/docs/tasks/administer-cluster/reconfigure-kubelet/), 必须一开始加上 `--dynamic-config-dir` 和开启 `DynamicKubeletConfig` feature gate （默认true）。
addons(coredns, kube-proxy): 更新对应的 ConfigMap

spec: https://godoc.org/k8s.io/kubelet/config/v1beta1#KubeletConfiguration

### 相关配置
`/etc/kubernetes/*.conf` 各组件访问 APIServer 鉴权文件。
`/etc/kubernetes/pki/` ca公钥私钥。
`/etc/kubernetes/manifests` 各组件静态 Pod。
`/var/lib/kubelet/kubeadm-flags.env` kubeadm 为 kubelet 设置的 `KUBELET_EXTRA_ARGS`。
`/var/lib/kubelet/config.yaml` kubelet 本地 configMap 配置文件。
`/etc/default/kubelet` kubelet `KUBELET_EXTRA_ARGS=` 配置。
`/etc/systemd/system/kubelet.service.d/10-kubeadm.conf` kubelet systemd 配置，启动命令行:

```bash
# Note: This dropin only works with kubeadm and kubelet v1.11+
[Service]
Environment="KUBELET_KUBECONFIG_ARGS=--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf"
Environment="KUBELET_CONFIG_ARGS=--config=/var/lib/kubelet/config.yaml"
# This is a file that "kubeadm init" and "kubeadm join" generates at runtime, populating the KUBELET_KUBEADM_ARGS variable dynamically
EnvironmentFile=-/var/lib/kubelet/kubeadm-flags.env
# This is a file that the user can use for overrides of the kubelet args as a last resort. Preferably, the user should use
# the .NodeRegistration.KubeletExtraArgs object in the configuration files instead. KUBELET_EXTRA_ARGS should be sourced from this file.
EnvironmentFile=-/etc/default/kubelet
ExecStart=
ExecStart=/usr/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_KUBEADM_ARGS $KUBELET_EXTRA_ARGS
```

## 其它插件
[nvidia-device-plugin](https://github.com/NVIDIA/k8s-device-plugin)
`kubectl apply -f https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/v1.11/nvidia-device-plugin.yml`

[dashboard](https://github.com/kubernetes/dashboard)
`kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml`

[rook](https://github.com/rook/rook)
```
kubectl apply -f https://raw.githubusercontent.com/rook/rook/master/cluster/examples/kubernetes/ceph/operator.yaml
kubectl apply -f https://raw.githubusercontent.com/rook/rook/master/cluster/examples/kubernetes/ceph/cluster.yaml
```

[kube-prometheus](https://github.com/coreos/prometheus-operator/tree/master/contrib/kube-prometheus)

[logging](https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/fluentd-elasticsearch)

## Tips
kubelet alias 自动补全(Linux)：
`source <(kubectl completion bash | sed 's/kubectl/kc/g')`

plugins ([krew](https://github.com/kubernetes-sigs/krew)): ctx/ns

better:
tail: [stern](https://github.com/wercker/stern)
debug: [kubectl-debug](https://github.com/aylei/kubectl-debug)

## 验证
### [Node Conformance Test](https://kubernetes.io/docs/setup/node-conformance/)
在 node 加入集群之前验证。
run: `test/e2e_node/conformance/run_test.sh`

### Cluster
validate cluster: `KUBECTL_PATH=$(which kubectl) NUM_NODES=1 KUBERNETES_PROVIDER=local cluster/validate-cluster.sh`

### 其他
https://github.com/heptio/sonobuoy
https://github.com/bloomberg/powerfulseal

# CRD
GroupKindVersion 标识一个资源，Kind 和 resource name 一致
ApiVersion: group/version

```
kubectl get <resource>.<version>.<group>
kubectl api-resources
kubectl api-versions
kubectl explain <resource> --api-version <apiVersion>
```

可以同时存在同一个资源的不同 served 版本，但只有一个当前 stored 版本，可以有多个历史 stored 版本 (`.status.storedVersions`)，其它版本由当前 stored 版本转化而来。
可以读取任意 served 版本，写入更新的是当前 stored 版本。
改变 stored 版本不影响已经存在的 object，只影响后续的创建和更新。

https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/
https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definition-versioning/

# Networking
Networking Model:

* all containers can communicate with all other containers without NAT
* all nodes can communicate with all containers (and vice-versa) without NAT
* the IP that a container sees itself as is the same IP that others see it as

## 网络方案
### Overlay (UDP/VXLAN)
[flannel](https://github.com/coreos/flannel)

UDP
[![img](https://cdn-images-1.medium.com/max/1600/1*JqSLd3cPv14BWDtE7YEcRA.png)](https://blog.laputa.io/kubernetes-flannel-networking-6a1cb1f8ec7c)

* flannel0: TUN 设备（三层，在应用程序 flanneld 和设备间传递 IP 包）
* docker0 网桥必须使用 flannel 为 node 分配的子网。
* flanneld 将原始 IP 包/二层帧封成 UDP 包（接收到时解包），并依据 etcd 里保存的 pod 和 node 地址信息更新路由表以转发到目标 node。
* 性能差：用户态内核态数据拷贝；用户态 UDP 解包封包。
* 为什么用 UDP 而不是 TCP？因为 UDP 高效，如果需要可靠性，则上层（内部封装的）协议会保证。为什么不用 IP？ 因为 IP 不能穿越 NAT，一些基于端口的负载均衡也无法完成。(VXLAN 类似)

VXLAN
[![img](https://msazure.club/content/images/2018/05/Flannel.jpg)](https://msazure.club/flannel-networking-demystify/)
VXLAN 实现的是一个虚拟的二层网络隧道，通过让处于同一个 VXLAN 网络（VNI相同）中的机器看似（逻辑上）处在同一个二层网络中（二层相通），而网络包转发的方式也类似二层网络中的交换机。只不过 FDB 从 MAC->目标端口变为 MAC->目标 IP，如：`e2:7a:57:13:65:4e dst 10.255.1.225 self permanent`

* flannel.1: VTEP(VXLAN Tunnel End Point)设备（ 二层，相当于 VNI:1 这个交换机的网口，过程类似 flannel0 + flanneld 转发）
* flanneld 下发维护:
  * node 上路由表：根据目标容器网络，找到网关为目标 node 上的 VTEP 节点，出口为本 VTEP 节点（每个 node 一个网段，本 VETP 节点和本机容器在同一子网，网关和出口 VTEP 在同一个 VXLAN 网络）: `10.244.14.0/24 via 10.244.14.0 dev flannel.1`
  * node 上 ARP 表：根据网关 VTEP 的 IP 地址，找到其 MAC 地址 (dest VTEP MAC)
  * flannel.1 上 FDB 表(`bridge fdb show dev flannel.1`)：根据网关 VTEP 的 MAC 地址，找到 VTEP 节点所在 node 的 IP 地址 (dest node IP)
* 封包格式（省略了source）：

```
| Outer Ethernet Header | Outer IP Header | Outer UDP Header | VXLAN Header | Inner Ethernet Header | Inner IP Header     |
| (dest node MAC)       | (dest node IP)  | (Port:4789)      | (VNI:1)      | (dest VTEP MAC)       | (dest container IP) |
```

容器内目标非本网段网关为网桥地址(cni0，通过veth pair)，本网段（其他容器）直接通过网桥转发。

### Routing (Host-GW)
![img](https://feisky.gitbooks.io/sdn/container/calico/calico.png)

[calico](https://www.projectcalico.org/)
路由： `目标 via 网关 dev 设备`，网关要和（出口）设备之间二层连通。

原理：
在 node 上添加目的 Pod 子网的路由（每个 node 一个网段），下一跳网关为目标 Pod 所在 node IP：`192.168.1.0/24 via 172.17.0.2 dev eth0`。
目的 node 上再通过路由转到目的 Pod 的 veth 上：`192.168.1.3 dev cali1234abc scope link`。
可以无需网桥设备，但网桥可以减少路由数量。

node 之间必须二层连通，否则需要使用 IPIP 模式：
当 node 之间三层通而二层不通时，须通过 tunl0 IP 隧道设备
`192.168.1.0/24 via 172.17.0.2 dev tunl0` 其中下一跳网关地址（目标 node IP）变为新 IP 包的目的地址，即通过三层转发了内部 IP 包。
或者：
添加 Pod 子网路由到 node 子网的网关(将 node 的网关也加入 BGP peer)，不过一般无权限修改。

比较：

* Host-GW: 本来二层就通
* IPIP: 变为 host IP 包，可以走中间路由
* VXLAN: 变为 host UDP 包，虚拟二层（MACinUDP）

容器内所有目标网关地址为: 169.254.1.1, MAC 为`ee:ee:ee:ee:ee:ee`: 使用网卡的 ARP 代理功能，当 ARP 请求目标跨网段时，网关设备收到此 ARP 请求，会用自己的 MAC 地址返回给请求者。这样网关的 IP 和 MAC 是什么并无影响。

组件：

[bird](http://bird.network.cz/): 使用 [BGP](https://www.projectcalico.org/why-bgp/) 协议，在节点之间共享路由信息。

  * node-to-node mesh: 每个 node 都是 BGP peer。
  * route reflector: 指定固定数量 node 为 BGP peer。其他 node 再和 reflector node 学习路由信息。

[felix](https://github.com/projectcalico/felix): 监听 etcd 中的网络配置，根据配置更新节点的路由和 iptables。
[confd](https://github.com/kelseyhightower/confd): 监听 etcd 的数据，实时更新 bird 的配置文件，并重新启动 bird 进程以加载最新配置。
[controller](https://github.com/projectcalico/kube-controllers): 使用独立的 etcd 需要，使其中的数据和 k8s 同步。
[cni](https://github.com/projectcalico/cni-plugin): k8s 网络管理插件（同时管理 `WorkloadEndpoint` 资源）（纯 docker 使用 [libnetwork](https://github.com/projectcalico/libnetwork-plugin)）。

 * calico 主插件
 * calico-ipam ip分配
 * `/etc/cni/net.d/10-calico.conflist`
```
{
    "name": "k8s-pod-network",
    "cniVersion": "0.3.0",
    "plugins": [
      {
        "type": "calico", # 插件名字
        "log_level": "info",
        "datastore_type": "kubernetes",
        "nodename": "node1",
        "mtu": 1440,
        "ipam": {
          "type": "host-local", # 使用本地 ipam 插件 而不是 calico-ipam
          "subnet": "usePodCidr"
        },
        "policy": {
            "type": "k8s"
        },
        "kubernetes": {
            "kubeconfig": "/etc/cni/net.d/calico-kubeconfig"
        }
      },
      {
        "type": "portmap",
        "snat": true,
        "capabilities": {"portMappings": true}
      }
    ]
}
```

资源：

* HostEndpoint: 对应 node 上的网络接口。
* WorloadEndpoint: 对应 pod 的 veth 网络接口（`cali*`），区分 namespace。
* Profile: 应用于某个单独的 endpoint，每个 endpoint 可以有多个 profile。
* NetworkPolicy: 通过 label selector 应用于一组 endpoint (label 从 pod 复制)，对应于 k8s 的 `NetworkPolicy`。

## [CNI](https://github.com/containernetworking/cni)
[spec](https://github.com/containernetworking/cni/blob/master/SPEC.md) / [k8s cni](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-node/kubelet-cri-networking.md)

功能：

* 在容器网络命名空间中配置网络接口。
* 在host上配置网络。
* 给容器中的网络接口分配IP地址以及路由。

[标准基础插件](https://github.com/containernetworking/plugins)：

* main: 创建网络设备, bridge, ipvlan, loopback, macvlan, ptp, vlan 等
* IPAM: IP地址分配, dhcp, host-local 等
* meta: 其他(内置flannel)

plugin位置：`/opt/cni/bin`
配置(Network Configuration Lists)：`/etc/cni/net.d` 只会加载按字母顺序第一个，如 `10-flannel.conflist`

执行环境：由 CRI (如dockershim) 调用: [`RunPodSandbox` -> `SetUpPod`](https://github.com/kubernetes/kubernetes/blob/4dc40aabfb9af68913986a2caf87906312c9f4ea/pkg/kubelet/dockershim/docker_sandbox.go#L177)

执行参数：

* 环境变量：`CNI_*`
  * `CNI_COMMAND`: `ADD`/`DEL`/`CHECK`/`VERSION`
  * `CNI_CONTAINERID`: 容器ID
  * `CNI_NETNS`: 网络命名空间路径, `/proc/<container_pid>/ns/net`
  * `CNI_IFNAME`: 容器内网络接口名字
  * `CNI_ARGS`: KV形式的自定义参数
  * `CNI_PATH`: plugin搜索路径
* stdin: `Network Configuration` 调用中可能会被修改
```
{
    "cniVersion": "0.4.0",
    "name": "dbnet",
    "type": "bridge",
    // type (plugin) specific
    "bridge": "cni0",
    "ipam": {
      "type": "host-local",
      // ipam specific
      "subnet": "10.1.0.0/16",
      "gateway": "10.1.0.1"
    },
    "dns": {
      "nameservers": [ "10.1.0.1" ]
    }
}
```
`delegate` 字段（内容和 `Network Configuration` 相同）可以用于委派其他插件完成。

执行结果(`ADD`)：

* result code: 1/2/3/11
* stdout: `Result` (`IPAM`的结果没有interfaces)
```
{
    "cniVersion": "0.4.0",
    "interfaces": [...],
    "ips": [...],
    "routes": [...],
    "dns": [...]
}
```
失败结果：
```
{
    "cniVersion": "0.4.0",
    "code": <numeric-error-code>,
    "msg": <short-error-message>,
    "details": <long-error-message> (optional)
}
```

整体List执行：
`ADD`按照`Network Configuration Lists`配置的plugins顺序执行，`DEL`反序执行。
```
{
    "cniVersion": "0.4.0",
    "name": "...",
    "disableCheck": false,
    "plugins": [<Network Configuration>, ...]
}
```

对于 `ADD`, 把结果 `Result` 作为`Network Configuration` 的一个新增字段 `prevResult` 传给下一个plugin。后面的 plugin 除非需要修改或覆盖结果，都应该把收到的 `prevResult` 原样输出给下一个。
如果 `ADD` 中有一个 plugin 失败，执行 `DEL` (全部 plugins) 作为错误处理。
`DEL` 输入也包含最近 `ADD` 的 `Result` 作为 `prevResult`。

# Storage
## [CSI](https://kubernetes-csi.github.io/)
[spec](https://github.com/container-storage-interface/spec/blob/master/spec.md) / [k8s csi](https://github.com/kubernetes/community/blob/master/contributors/design-proposals/storage/container-storage-interface.md)

[![img](https://cdn-images-1.medium.com/max/1000/1*oMgMPjx0obXKlaItZOkRfA.png)](https://medium.com/google-cloud/understanding-the-container-storage-interface-csi-ddbeb966a3b)

external components (从 kubelet 剥离，由 k8s 维护):

* [node driver registrar](https://github.com/kubernetes-csi/node-driver-registrar): [registering] 向 kubelet 注册 CSI 驱动（利用 deivce plugin/KubeletPluginsWatcher 的注册机制），触发 CSI Identity 端点的 `GetPluginInfo/GetPluginCapabilities` 方法。安装驱动可执行文件。以及健康检测。
* [cluster driver registrar](https://github.com/kubernetes-csi/cluster-driver-registrar): 为驱动创建 `CSIDriver` 对象。自定义一些行为（attachRequired/podInfoOnMountVersion）。（非必要）
* [external provisioner](https://github.com/kubernetes-csi/external-provisioner): [provisioning] 监听 `PersistentVolumeClaim` 对象，触发 CSI 端点的 `CreateVolume/DeleteVolume` 方法创建实际的卷，进而创建出对应的 `PersistentVolume`。`StorageClass.provisioner` 要和 CSI 端点 `GetPluginInfo` 返回的一致。
* [external attacher](https://github.com/kubernetes-csi/external-attacher): [attaching] 监听 `VolumeAttachment` 对象（当 pod 调度到 node 时，由 kube-contoller-manager 创建），触发 CSI Controller 端点的 `ControllerPublishVolume/Unpublish` 方法，更新 `VolumeAttachment` 状态为 `Attached`。

挂载的一般流程：(registering) -> provisioning -> attaching -> mounting
[mounting] kubelet 监听 `VolumeAttachment` 的状态，当为 `Attached` 时，触发 CSI Node 端点的 `NodeStageVolume/NodeUnstageVolume`格式化准备和 `NodePublishVolume/NodeUnpublishVolume` 方法进行实际的挂载。

csi driver / plugin （第三方提供）：

* CSI Identity: 提供驱动的标识和基本信息。

```
service Identity {
  rpc GetPluginInfo(GetPluginInfoRequest) returns (GetPluginInfoResponse) {}
  rpc GetPluginCapabilities(GetPluginCapabilitiesRequest) returns (GetPluginCapabilitiesResponse) {}
  rpc Probe (ProbeRequest) returns (ProbeResponse) {}
}
```

* CSI Controller: 提供卷的基本服务：创建，删除，attach/dettach (publish/unpublish，指将设备挂到node上，一般只有块存储需要), snapshoting 等。（volume contoller逻辑，不需要在node上操作，部署在master节点上）。

```
service Controller {
  rpc CreateVolume (CreateVolumeRequest) returns (CreateVolumeResponse) {}
  rpc DeleteVolume (DeleteVolumeRequest) returns (DeleteVolumeResponse) {}
  rpc ControllerPublishVolume (ControllerPublishVolumeRequest) returns (ControllerPublishVolumeResponse) {}
  rpc ControllerUnpublishVolume (ControllerUnpublishVolumeRequest) returns (ControllerUnpublishVolumeResponse) {}
  rpc ValidateVolumeCapabilities (ValidateVolumeCapabilitiesRequest) returns (ValidateVolumeCapabilitiesResponse) {}
  rpc ListVolumes (ListVolumesRequest) returns (ListVolumesResponse) {}
  rpc GetCapacity (GetCapacityRequest) returns (GetCapacityResponse) {}
  rpc ControllerGetCapabilities (ControllerGetCapabilitiesRequest) returns (ControllerGetCapabilitiesResponse) {}
  rpc CreateSnapshot (CreateSnapshotRequest) returns (CreateSnapshotResponse) {}
  rpc DeleteSnapshot (DeleteSnapshotRequest) returns (DeleteSnapshotResponse) {}
  rpc ListSnapshots (ListSnapshotsRequest) returns (ListSnapshotsResponse) {}
  rpc ControllerExpandVolume (ControllerExpandVolumeRequest) returns (ControllerExpandVolumeResponse) {}
}
```

* CSI Node: 在node上执行的具体操作（由 kubelet 直接调用），如格式化以及 mount/unmount 存储路径到 pod volume 的 mount point 上。

```
service Node {
  rpc NodeStageVolume (NodeStageVolumeRequest) returns (NodeStageVolumeResponse) {}
  rpc NodeUnstageVolume (NodeUnstageVolumeRequest) returns (NodeUnstageVolumeResponse) {}
  rpc NodePublishVolume (NodePublishVolumeRequest) returns (NodePublishVolumeResponse) {}
  rpc NodeUnpublishVolume (NodeUnpublishVolumeRequest) returns (NodeUnpublishVolumeResponse) {}
  rpc NodeGetVolumeStats (NodeGetVolumeStatsRequest) returns (NodeGetVolumeStatsResponse) {}
  rpc NodeExpandVolume(NodeExpandVolumeRequest) returns (NodeExpandVolumeResponse) {}
  rpc NodeGetCapabilities (NodeGetCapabilitiesRequest) returns (NodeGetCapabilitiesResponse) {}
  rpc NodeGetInfo (NodeGetInfoRequest) returns (NodeGetInfoResponse) {}
}
```

部署：
![img](https://raw.githubusercontent.com/kubernetes/community/master/contributors/design-proposals/storage/container-storage-interface_diagram1.png)

(StatefulSet/Deployment) external-attacher/provisioner 通过 emptyDir 里的由 CSI Driver 创建的 socket 和其通信。
(DaemonSet) node-driver-registrar 通过 hostpath `/var/lib/kubelet/plugins_registry/[SanitizedCSIDriverName]-reg.sock` 和 kubelet 通信。
kubelet/resgistrar 通过 hostpath `/var/lib/kubelet/plugins/[SanitizedCSIDriverName]/csi.sock` 和 CSI Driver 通信。
`/var/lib/kubelet/pods/` 必须以 `mountPropagation: "Bidirectional"` 挂载进 CSI Driver，这样 CSI Driver 里的挂载在 host 上及用户容器里才能看到。

快照：
创建指定 PVC 的快照 `VolumeSnapshot`，(由 `VolumeSnapshotClass` 的 csi 插件处理 `CreateSnapshot`），内容为 `VolumeSnapshotContents`，类比 PVC 和 PV 的关系。
使用：创建一个 `spec.dataSource` 指定为对应快照的 PVC，csi 插件将快照内容复制到对应 PV 上 `CreateVolume`。
`VolumeSnapshot` 的 `status.ReadyToUse` 为 true 时，说明快照已完成，源 PVC 可以删除。


# Runtime
## [CRI](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-node/container-runtime-interface.md)

```
service RuntimeService {
    rpc Version(VersionRequest) returns (VersionResponse) {}
    rpc RunPodSandbox(RunPodSandboxRequest) returns (RunPodSandboxResponse) {}
    rpc StopPodSandbox(StopPodSandboxRequest) returns (StopPodSandboxResponse) {}
    rpc RemovePodSandbox(RemovePodSandboxRequest) returns (RemovePodSandboxResponse) {}
    rpc PodSandboxStatus(PodSandboxStatusRequest) returns (PodSandboxStatusResponse) {}
    rpc ListPodSandbox(ListPodSandboxRequest) returns (ListPodSandboxResponse) {}
    rpc CreateContainer(CreateContainerRequest) returns (CreateContainerResponse) {}
    rpc StartContainer(StartContainerRequest) returns (StartContainerResponse) {}
    rpc StopContainer(StopContainerRequest) returns (StopContainerResponse) {}
    rpc RemoveContainer(RemoveContainerRequest) returns (RemoveContainerResponse) {}
    rpc ListContainers(ListContainersRequest) returns (ListContainersResponse) {}
    rpc ContainerStatus(ContainerStatusRequest) returns (ContainerStatusResponse) {}
    rpc UpdateContainerResources(UpdateContainerResourcesRequest) returns (UpdateContainerResourcesResponse) {}
    rpc ReopenContainerLog(ReopenContainerLogRequest) returns (ReopenContainerLogResponse) {}
    rpc ExecSync(ExecSyncRequest) returns (ExecSyncResponse) {}
    rpc Exec(ExecRequest) returns (ExecResponse) {}
    rpc Attach(AttachRequest) returns (AttachResponse) {}
    rpc PortForward(PortForwardRequest) returns (PortForwardResponse) {}
    rpc ContainerStats(ContainerStatsRequest) returns (ContainerStatsResponse) {}
    rpc ListContainerStats(ListContainerStatsRequest) returns (ListContainerStatsResponse) {}
    rpc UpdateRuntimeConfig(UpdateRuntimeConfigRequest) returns (UpdateRuntimeConfigResponse) {}
    rpc Status(StatusRequest) returns (StatusResponse) {}
}

ervice ImageService {
    rpc ListImages(ListImagesRequest) returns (ListImagesResponse) {}
    rpc ImageStatus(ImageStatusRequest) returns (ImageStatusResponse) {}
    rpc PullImage(PullImageRequest) returns (PullImageResponse) {}
    rpc RemoveImage(RemoveImageRequest) returns (RemoveImageResponse) {}
    rpc ImageFsInfo(ImageFsInfoRequest) returns (ImageFsInfoResponse) {}
}
```

```
[kubelet] -- (CRI) -- [dockershim] -- [docker daemon] -- [containerd] -- (OCI) -- [runc]
                   -- [cri-containerd] ------------------^               ^
                   -- [cri-o] -------------------------------------------'
```

默认 kubelet (`--container-runtime=docker`) 通过 `--container-runtime-endpoint=/var/run/dockershim.sock`（image 默认和 runtime 相同） 和实现了 CRI 的 dockershim GRPC server 通信。
若使用 cri-o, 则配置 `--container-runtime=remote` 及 `--container-runtime-endpoint=/var/run/crio/crio.sock`。

在 docker runtime 下，Exec/Attach/PortForward 会被重定向到 kubelet 的 `/cri/` 路径下，由 streaming server 处理（可以是独立 HTTP server，实际直接注册 handler 到 kubelet 中），最终由实现了 `Runtime` 接口的 `streamingRuntime` 执行。
```
type Runtime interface {
	Exec(containerID string, cmd []string, in io.Reader, out, err io.WriteCloser, tty bool, resize <-chan remotecommand.TerminalSize) error
	Attach(containerID string, in io.Reader, out, err io.WriteCloser, tty bool, resize <-chan remotecommand.TerminalSize) error
	PortForward(podSandboxID string, port int32, stream io.ReadWriteCloser) error
}
```
即 `apiserver -(exec cmd)-> kubelet -(cri.Exec)-> dockershim -(redirect /cri/exec/)-> kubelet -(runtime.Exec)-> streamingServer -(exec container)-> container`


`kubeGenericRuntimeManager.SyncPod`

1. 创建并启动 sandbox 容器，使用 pause 镜像，调用 CNI 插件配置网络。
1. 按顺序创建并启动 init 容器。
1. 创建并启动普通容器。默认情况下容器共享 sandbox 的 Network/Ipc namespace。

## [cri-o](https://cri-o.io/)
* kubelet 运行 pod，pod 里的容器共享同样的 IPC/NET/PID namespace 和 cgroup。
* kubelet 通过 CRI 接口向 CRI-O daemon 发送运行 pod 的请求。
* CRI-O 使用 `containers/image` 库从 registry 拉取镜像。
* CRI-O 使用 `containers/storage` 库将镜像解压成 rootfs，存放于 COW 文件系统里。
* 当容器的 rootfs 创建后，CRI-O 使用 OCI 生成工具生成一份描述如何运行容器的 OCI 运行时 json 配置。
* CRI-O 启动 OCI 兼容的运行时（默认 [runc](https://github.com/opencontainers/runc)）根据配置来运行容器进程。
* 每个容器都被一个独立的 conmon 进程监控，监控进程还持有容器进程(PID1)的 pty 终端，处理日志和记录容器进程的退出码。
* Pod 的网络由 CNI 设置，CRI-O 可以使用任何 CNI 插件。

# Components
## APIServer

## [Scheduler](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-scheduling/scheduler.md)
1. 通过 Predicates 过滤出符合条件的 node。
1. 通过 Priorities 为符合条件的 node 打分。`finalScoreNodeA = (weight1 * priorityFunc1) + (weight2 * priorityFunc2)`
1. 选择最高分的那个 node 作为目标，设置 Pod 的 `spec.nodeName`。

### [Predicates & Priorities](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-scheduling/scheduler_algorithm.md)
Predicates
GeneralPredicates (也会被 kubelet 启动 Pod 前再次检查)：

`PodFitsResources`: 资源是否满足 [node]capacity - sum([Pod]spec.resources.requests)
`PodFitsHost`: node 名字是否和 `spec.nodeName` 一致
`PodFitsHostPorts`: host port 在 node 上是否已被占用
`PodMatchNodeSelector`: nodeSelector 或 nodeAffinity 是否和 node 匹配

Volume 相关：
`NoDiskConflict`: Pod 声明挂载的持久化 Volume 与 node 上已挂载的是否有冲突
`MaxPDVolumeCountPredicate`: node 持久化 Volume 的最大数目
`VolumeZonePredicate`: 持久化 Volume 的 Zone 标签，是否与 node 的 Zone 匹配
`VolumeBindingPredicate`: Pod 的 PVC 对应的 PV nodeAffinity, 是否与 node 匹配（没绑定 PV 的则检查是否有符合的 PV）

`PodToleratesNodeTaints`: Pod 是否能 tolerate node 的 taint
`NodeMemoryPressurePredicate` 检查 node 内存是否已经不够
`PodAffinityPredicate`: Pod 与 node 上已有 Pod 间的 affinity 和 anti-affinity

Priorities
`LeastRequestedPriority`: 选择空闲 CPU 和 Memory 最多的 node `score = (cpu((capacity-sum(requested))10/capacity) + memory((capacity-sum(requested))10/capacity))/2`
`BalancedResourceAllocation`:  选择调度完成后 node 各种资源分配最均衡的 `score = 10 - variance(cpuFraction,memoryFraction,volumeFraction)*10`
`NodeAffinityPriority`, `TaintTolerationPriority`, `InterPodAffinityPriority`: 选择匹配字段最多的
`ImageLocalityPriority`: 选择已经有镜像缓存的 node

### Priority & Preemption
使用：创建 `PriorityClass` 对象，在 Pod 里设置 `spec.priorityClassName` 指定。值越大越高，未设置默认`globalDefault`，Pod 未声明使用的话则 priority 为0。
调度时，优先级高的 Pod 先出队列(`activeQ`)优先调度。

抢占：当一个 Pod 调度失败时（放入`unschedulableQ`），调度器找一个 node，当 node 上一个或多个比其优先级底的 Pod 删除之后，它可以调度上去。先设置 `spec.nominatedNodeName` 为目标 node（放回`activeQ`，进入下一调度周期），然后删除 node 上的低优先级的 Pod，此时如果有更高优先级的 Pod 也来抢占这个 node，原 Pod 的`spec.nominatedNodeName`就被清除。考虑到 pod affinity, 调度新 Pod 时也需要考虑 node 上也有（或者没有）抢占 Pod 的情况（nominatedNodeName 指定了此 node）。

### Scheduling Framework
[![img](https://raw.githubusercontent.com/kubernetes/enhancements/master/keps/sig-scheduling/20180409-scheduling-framework-extensions.png)](https://github.com/kubernetes/enhancements/blob/master/keps/sig-scheduling/20180409-scheduling-framework.md)

扩展点（由一系列插件组成）：在每个调度周期（scheduling context），扩展点顺序执行。但不同周期的绑定阶段的插件可能并行。因为绑定阶段是到另一个 goroutine 中异步执行。

调度阶段：顺序执行

* QueueSort: 对调度队列中的 pod 进行排序
* Pre-Filter: 预过滤，检查一些集群或 pod 必须满足的条件
* Filter: 类似 Predicate
* Post-Filter: 后过滤，可以用来更新内部状态或日志
* Scoring: 类似 Priority
* Normalize-Scoring: 使 Scoring 的值正规化到规定的值域范围
* Reserve: 有状态的插件利用来预留 node 给这个 pod 的资源（assume，预绑定）

绑定阶段：可能会并发执行

* Permit: 同意/拒绝/等待
  * approve: 所有插件 approve
  * deny: 只要有一个插件 deny
  * wait: 在此阶段等待直到有一个插件 approve
* Pre-Bind: 准备绑定，如在 node 上 provision 工作
* Bind: 只能有一个插件进行实际的绑定
* Post-Bind: 用来清理相关资源
* Un-Reverse: 当 pod 被 Reserve，然后被拒绝后才会调用

自定义扩展：在主程序中注册，并在配置文件中 enable。
```go
func main() {
	command := app.NewSchedulerCommand(func(r framework.Registry) error {
		r.Register(mypl.Name, mypl.New)
		return nil
	})

	if err := command.Execute(); err != nil {
		os.Exit(1)
	}
}
```
同一个插件可以注册在不同扩展点（同一实例）。不同调度周期，通过插件内部状态共享信息，同一个调度周期内，（不同插件）通过 `CycleState` 共享信息。

## [Controller](https://git.k8s.io/community/contributors/devel/sig-api-machinery/controllers.md)
### [client-go](https://github.com/kubernetes/client-go/)
[![img](https://raw.githubusercontent.com/kubernetes/sample-controller/master/docs/images/client-go-controller-interaction.jpeg)](https://github.com/kubernetes/sample-controller/blob/master/docs/controller-client-go.md)

client-go 库内部组件：

* Reflector: 从 Kubernetes API 监听 (watch) 特定的资源类型 (kind)，可以是内置资源，也可以是自定义资源。当它收到新增资源对象的通知时，会把新的资源对象从 APIServer 取出 (list) 并放入 Delta Fifo 队列。
* Informer: 从 Delta Fifo 队列中取出对象放入本地 Indexer 缓存，然后调用自定义控制器注册的事件处理函数。定期同步功能(resync)，将 Indexer 中已知的对象(knownObjects，就是 Indexer 的引用)重新放入 Delta Fifo 中，以重新通知 Event Handler 处理。
* Indexer: 提供对象的索引功能，可以根据对象的标签创建索引提高对象查询效率。它使用一个线程安全的数据存储作为后端缓存存储对象和其键值（一般为`<namespace>/<name>`）。

自定义控制器组件：

* Resource Event Handlers: 注册在 Informer 里的针对特定资源的事件处理回调函数，通常作法是将对象的键值放入 Work Queue 以供后续处理。之所以只存键值，是因为 Indexer 里的缓存的对象可能被更新，这样处理时可以取得最新的对象。
* Work Queue: 将对象的投递和处理分离开，异步处理，起到了解藕的作用。
* Object Sync Handler: 从 Work Queue 里取出对象的键值，以从 Indexer 获得实际的对象进行处理。

operator best practices:
https://github.com/slaise/community-operators/blob/master/docs/best-practices.md

## [Device Plugin](https://github.com/kubernetes/community/blob/master/contributors/design-proposals/resource-management/device-plugin.md)

![img](https://raw.githubusercontent.com/kubernetes/community/master/contributors/design-proposals/resource-management/device-plugin-overview.png)
DP 通过 Daemonset 部署在每个 node 上。

DP 通过 `/var/lib/kubelet/device-plugins/kubelet.sock` 向 kubelet 注册:

 * DP 的版本
 * 端点名字，在 `/var/lib/kubelet/device-plugins/` 下
 * 资源名字 (Extended Resource)

```
service Registration {
	rpc Register(RegisterRequest) returns (Empty) {}
}
```

kubelet 通过 `/var/lib/kubelet/device-plugins/{注册的端点名字}` 向 DP 请求：
```
service DevicePlugin {
	// returns a stream of []Device
	rpc ListAndWatch(Empty) returns (stream ListAndWatchResponse) {}
	rpc Allocate(AllocateRequest) returns (AllocateResponse) {}
}
```

`ListAndWatch`: 返回一组设备，包括其 ID 和健康状况。
`Allocate`: 请求一组设备（请求哪些空闲的设备是 kubelet 决定的），返回其在容器内的运行时设置。运行时设置包括: 一组环境变量，需要从 node 上 mount 的一组目录（通常包括驱动和工具文件），需要 mount 的具体设备文件。


DP 可以通过 `/var/lib/kubelet/pod-resources/kubelet.sock` 向 kubelet 查询设备资源在 node 上的分配情况。
```
// PodResources is a service provided by the kubelet that provides information about the
// node resources consumed by pods and containers on the node
service PodResources {
    rpc List(ListPodResourcesRequest) returns (ListPodResourcesResponse) {}
}
```

KubeletPluginsWatcher: 被动注册机制，CSI/CNI 等插件使用。
kubelet 通过监控 `/var/lib/kubelet/plugins_registry/` 里面的端点向 plugin 请求进行注册：
```
service Registration {
	rpc GetInfo(InfoRequest) returns (PluginInfo) {}
	rpc NotifyRegistrationStatus(RegistrationStatus) returns (RegistrationStatusResponse) {}
}
```
PluginInfo 返回了插件类型（CSIPlugin, DevicePlugin）和插件实际 endpoint（一般在 `/var/lib/kubelet/plugins/`），后续对 plugin 的请求（如 CSI.Identity）会向这个 endpoint 发送。

# Resources
`kubectl api-resources`

## Pod
### namesapce
默认情况下容器共享 sandbox 的 Network/Ipc namespace，共享 host 的 Cgroup/User namespace，创建独立的 PID/Mount/UTS namespace。

`spec.hostNetwork` 是否和 host 共享 Network/UTS namespace。
`spec.hostIPC` 是否和 host 共享 IPC namespace。
`spec.hostPID` 是否和 host 共享 PID namespace。
`spec.shareProcessNamespace` 是否和 sandbox 共享 PID namespace。

### status
phase

* Pending: 没有容器启动，可能还未绑定到 node 等待调度，或者在拉镜像
* Running: 所有容器已经启动，至少有一个容器在运行或在重启
* Succeed: 所有容器成功退出，而不重启
* Failed: 所有容器已终止，但至少有一个退出码不为0
* Unknown: 不能获取状态，通常是通信问题

condition

* PodScheduled: 已进行调度
* Ready: 可以提供服务，加入 service 负载平衡
* Initialized: init 容器成功完成
* Unschedulable: 未能成功调度
* ContainersReady: 所有容器已 ready

### PLEG
[![img](https://raw.githubusercontent.com/kubernetes/community/master/contributors/design-proposals/node/pleg.png)](https://github.com/kubernetes/community/blob/master/contributors/design-proposals/node/pod-lifecycle-event-generator.md)

kubelet 需要根据 pod spec 的变化同步容器状态，也要根据容器状态同步到 pod status。每个 pod 都有自己的同步 goroutine worker，同时周期性请求 container runtime 会造成性能问题，PLEG 就作为单独模块来查询 (relist) container runtime 作为代替，并将容器的事件转为 pod 事件发送给 kubelet 处理，kubelet 会再根据事件 ID 派给各个 pod worker 处理。 relist 也可能对性能有影响，所以也接受一个事件流。

pod 事件类型：
```golang
// ContainerStarted - event type when the new state of container is running.
ContainerStarted PodLifeCycleEventType = "ContainerStarted"
// ContainerDied - event type when the new state of container is exited.
ContainerDied PodLifeCycleEventType = "ContainerDied"
// ContainerRemoved - event type when the old state of container is exited.
ContainerRemoved PodLifeCycleEventType = "ContainerRemoved"
// PodSync is used to trigger syncing of a pod when the observed change of
// the state of the pod cannot be captured by any single event above.
PodSync PodLifeCycleEventType = "PodSync"
// ContainerChanged - event type when the new state of container is unknown.
ContainerChanged PodLifeCycleEventType = "ContainerChanged"
```

## Deployment
部署策略

[![img](https://raw.githubusercontent.com/ContainerSolutions/k8s-deployment-strategies/master/decision-diagram.png)](https://container-solutions.com/kubernetes-deployment-strategies/)

* 重建 recreate: 先删掉旧的版本，再发布新的版本。`.spec.strategy.type==Recreate`
* 滚动更新 ramped: 一个接一个更新，更新时新旧版本共存，本质上就是同时更新两个版本的 replicaset 中 pod 的数目。 `.spec.strategy.type==RollingUpdate`（默认）
  * maxSurge: 允许除 desired 外创建多余的新 pod 数目，默认 %25 desired。当为0，只有先删一个 pod，才能创建一个 pod。
  * maxUnavailable: 最多不可用的 pod 数目，默认 %25 desired。当为0，只有先创一个 pod，才能删一个 pod。所以和 maxSurge 不能同时为0。
* 蓝/绿 blue/green: 先发布新版本，再将流量从旧版本切到新版本，最后删掉旧版本。通过 service.selector 更新不同版本切换两个 deployment。
* 金丝雀 canary（灰度，手动 RollingUpdate）: 先发布新版本，再将部分流量切到新版本，最后再切全量。使用两个 deployment，设置不同的 replicas 比例，一个 Service 选中两个 deployment 所有 pod（或使用一个 Statefulset，用 partition 区分）。
* a/b 测试: 类似 canary，但以更精确的方式（比如 HTTP 头）针对发布。利用 Istio, Nginx 等控制。
* 阴影 shadow: 新版本和旧版本共存，但只接收少部分流量，且不影响回应。可以通过 Istio 复制请求等方式。

用 `spec.revisionHistoryLimit` 控制历史版本数 （旧`ReplicaSet`数目）。不同的 template-hash 就是不同的版本。
`minReadySeconds`: pod ready 再加上这个时间才算做 available 状态。

## StatefulSet
指定 `.spec.serviceName` 为对应的 Headless Service。

* 利用 Headless Service，每个 pod 的 DNS 域名：`<statefulset-name>-<seq>.<svc-name>.<namespace>.svc.cluster.local`。前缀对应到同名 pod 稳定不变，所以网络标识也是不变稳定的。
* 如果申请了持久存储，名字也一一和 pod 对应：`<pvc-name>-<statefulset-name>-<seq>`
* 创建和删除 pod 时，严格按照编号顺序（删除反序）。

更新策略：`.spec.updateStrategy.type`

* `RollingUpdate`（默认）: 滚动更新，按照删除的顺序（2->1->0）更新，每次更新一个 pod。相当于 maxSurge 为 0。
  * `partition`: 分区（灰度），不全部更新，只有序号大于等于其值的才会更新，即保留旧版本的数量。
* `OnDelete`: 不更新，必须手动删除老 pod（或缩容再扩容），让 controller 创建新 pod 来更新。

每个 pod 有个 `controller-revision-hash` label，标识属于那个版本（`ControllerRevision`）。

扩缩容策略: `.spec.podMangementPolicy`

* `orderedReady`: （默认）按顺序等上一个及其前面所有 pod ready 之后才创建新的。
* `parallel`： 不需要等 pod ready，创建出一个，就可以继续创建下一个。

## DaemonSet
通过 `nodeAffinity` 和 `toleration` 保证每个 node 上运行一个 pod 实例。

DaemonSet 和 StatefulSet 都通过 `ControllerRevision`（保存了历史版本内容） 进行版本控制，而 Deployment 是直接通过 `ReplicaSet`。

## Job & CronJob
Job 通过 controller 生成的 `controller-uid=` 作为 selector，控制选中的 pod，是为了保证一个 Job 和其控制 pod 的唯一对应关系（新 Job 不应再控制旧的正在运行或已完成的 pod）。相比 Deployment/STS/DS 使用用户定义的 label 作为 selector，因为是 RLS 不要求这种唯一对应关系。

restartPolicy 在 Job 对象里只允许被设置为 Never/OnFailure, 在 Deployment 里只能是 Always。

* 当 Nerver 时，用 `spec.backoffLimit` 控制失败次数（重新创建 pod 次数）。
* `spec.activeDeadlineSeconds` 控制运行超时，也可以防止 OnFailure 时一直重启。

并发控制

* 不需要并发：不设置 `.spec.completions` 和 `.spec.parallelism`，默认都为1。
* 固定成功完成数：设置 `.spec.completions`，不设置 `.spec.parallelism` （默认为1，可设大于1）。
* 工作队列：不设置 `.spec.completions`，只设置 `.spec.parallelism`。完成数不能确定，只要有一个 pod 成功，Job 就算成功（其他 pod 应该自动退出）。

CronJob 是定时任务，控制 Job。
`spec.concurrencyPolicy`: Allow Job可以同时存在；Forbid 跳过，Job不会被创建；Replace 新 Job 替换旧 Job。

## Service & Endpoints
每个 Service 有个对应同名 Endpoints，包含被选中的 Pod 地址列表和状态，其中只有 ready 的 Pod 才会被 Service 选中转发。
Pod 通过 Service 访问到自己需要 kubelet 开启 --hairpin-mode 为 promiscuous-bridge（默认）或 hairpin-veth。

### ClusterIP
DNS 服务名解析到 VIP A 记录。

iptables: (VIP: 10.96.0.1, PodIP: 192.168.99.101)
```bash
-A PREROUTING -m comment --comment "kubernetes service portals" -j KUBE-SERVICES
-A KUBE-SERVICES -d 10.96.0.1/32 -p tcp -m comment --comment "default/kubernetes:https cluster IP" -m tcp --dport 443 -j KUBE-SVC-NPX46M4PTMTKRN6Y
-A KUBE-SVC-NPX46M4PTMTKRN6Y -j KUBE-SEP-6QO3ZMTIB4FPXHNT # 如果有多个 endpoints，用 –mode random 负载均衡
-A KUBE-SEP-6QO3ZMTIB4FPXHNT -s 192.168.99.101/32 -j KUBE-MARK-MASQ # 没有匹配，不 SNAT
-A KUBE-MARK-MASQ -j MARK --set-xmark 0x4000/0x4000
-A KUBE-SEP-6QO3ZMTIB4FPXHNT -p tcp -m tcp -j DNAT --to-destination 192.168.99.101:6443 # 返回包自动 un-dnat
```

iptables 规则由 kube-proxy 负责维护，参考 [kube-proxy iptables 模式源码分析](https://blog.tianfeiyu.com/2019/11/06/kube_proxy_iptables/)。

当服务多了之后，iptables 做负载均衡有性能问题，可以使用 [IPVS 负载均衡](https://github.com/kubernetes/kubernetes/blob/master/pkg/proxy/ipvs/README.md)。

Headless Service: `spec.ClusterIP: None`

* 没有 VIP，DNS 服务名直接解析为选中的 pod IP 的多个 A 记录。（没有 iptables 规则）
* 每个选中的 pod 有唯一网络标识：`<pod-name>.<svc-name>.<namespace>.svc.cluster.local`，可以用 Pod 的 hostname, subdomain 自定义。

### NodePort
iptables: 不匹配 ClusterIP 的都转到 KUBE-NODEPORTS 再转到对应 svc
```
...
-A KUBE-SERVICES -m comment --comment "kubernetes service nodeports; NOTE: this must be the last rule in this chain" -m addrtype --dst-type LOCAL -j KUBE-NODEPORTS
-A KUBE-NODEPORTS -p tcp -m comment --comment "default/mongo:" -m tcp --dport 31958 -j KUBE-MARK-MASQ
-A KUBE-NODEPORTS -p tcp -m comment --comment "default/mongo:" -m tcp --dport 31958 -j KUBE-SVC-G2OJTDIWIJ7HQ7MY

-A POSTROUTING -m comment --comment "kubernetes postrouting rules" -j KUBE-POSTROUTING
-A KUBE-POSTROUTING -m comment --comment "kubernetes service traffic requiring SNAT" -m mark --mark 0x4000/0x4000 -j MASQUERADE # 如果被 LB 到别的 node 上要 SNAT，否则响应不过来直接发回 client
```

kube-proxy 会在 node 上预留占用所有地址对应的端口（不走流量）。

### [Source IP 问题](https://kubernetes.io/docs/tutorials/services/source-ip/)
kube-proxy 使用 iptable 的情况，通过下面方式访问

* ClusterIP: 不做 SNAT (除了 ExternalIP)
* NodePort: SNAT 成对应 node 的地址，可以将`service.spec.externalTrafficPolicy`设为`Local`（默认`Cluster`）只发本地 node 上的 endpoint 来避免（要求 node 上有对应 endpoint，否则 drop），但因为不像`Cluster`那样对所有endpoint全局轮发，可能导致负载不均衡。
* LoadBalancer: 情况和 NodePort 类似，如果`service.spec.externalTrafficPolicy`设为`Local`，LB只会转发流量到有对应 endpoint 的 node 上（通过 health check `service.spec.healthCheckNodePort`）。

### ExtenalName
将服务名直接 CNAME 到 ExtenalName。

### ExtenalIP
通过 externalIP (一般为公网 IP) 加 service port 访问，extenalIP 至少能路由（必须是 dst ip，不能经过 NAT）到一个集群内 node 结点。

iptables: (ExternalIP: 10.0.2.15)
```
-A KUBE-SERVICES -d 10.0.2.15/32 -p tcp -m comment --comment "default/mongo: external IP" -m tcp --dport 27017 -j KUBE-MARK-MASQ
-A KUBE-SERVICES -d 10.0.2.15/32 -p tcp -m comment --comment "default/mongo: external IP" -m tcp --dport 27017 -m physdev ! --physdev-is-in -m addrtype ! --src-type LOCAL -j KUBE-SVC-G2OJTDIWIJ7HQ7MY
-A KUBE-SERVICES -d 10.0.2.15/32 -p tcp -m comment --comment "default/mongo: external IP" -m tcp --dport 27017 -m addrtype --dst-type LOCAL -j KUBE-SVC-G2OJTDIWIJ7HQ7MY
```

kube-proxy 会在 node 上预留占用 externalIP 地址对应的端口（不走流量）。

## Ingress
HTTP/HTTPS Service 的反向代理
ingress rule: host (:ingress controller port) -> Services(:service port)

## PersistentVolume & PersistentVolumeClaim & StorageClass
PVC 和 PV 一一绑定：PV 的 spec （容量等）满足 PVC 的要求；storage class 一致（StorageClass 可以设置缺省值`storageclass.kubernetes.io/is-default-class`）。

provisioning:
静态 provisioning:（StorageClass provisioner=kubernetes.io/no-provisioner）手动创建 PV。
动态 provisioning:（StorageClass 指定了具体的 provisoner）: PVC 指定 storage class 名字，provisoner 根据 PVC 的要求及 StorageClass 的属性，创建出 PVC 对应的 PV。
绑定执行：kube-controller-manager `PersistentVolumeController`

StorageClass 可以指定 `allowedTopologies` 以限制创建出来的 PV 的 topo 位置，使用它的 pod 调度的时候只能调度到符合 topo 的节点上。

attaching:
为 node 挂载磁盘（一般为块设备，其他比如分布式文件系统 NFS，CephFS 并不需要）。
执行：kube-controller-manager (`AttachDetachController`调用具体 volume plugin，如rbd, csi...的`Attacher.Attach`)

mounting:
格式化磁盘（或直接用远程存储）并 mount 到 pod 的 volume 目录 `/var/lib/kubelet/pods/<Pod-ID>/volumes/kubernetes.io~<Volume-Type>/<Volume-Name>`。
执行：kubelet (`VolumeManager`调用具体 volume plugin 的 `DeviceMounter.MountDevice`格式化等准备工作和 `Mounter.SetUp`具体挂载)

### local PV
```
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: local-storage
provisioner: kubernetes.io/no-provisioner
volumeBindingMode: WaitForFirstConsumer
```
不支持动态 provisioning: 需要手动创建 PV（指定节点信息），可以利用 [local-static-provisioner](https://github.com/kubernetes-sigs/sig-storage-local-static-provisioner)自动创建 PV（当对应绑定的 PVC 删除后，对应 PV 会清除并重建）。
延迟绑定: PVC 和 PV 不立即绑定，等到第一个使用 PVC 的 Pod 调度的时候，由调度器综合考虑 PV 节点情况再绑定。（删除 Pod 后，PVC 和 PV 还是绑定状态，若不删除，下次 Pod 还是会调度到这个节点上）

如果是支持动态 provisioning 的，也可做延迟创建（并绑定）。

## NetworkPolicy
podSelector: 被选中的处于 Deny All，在 ingress, egress 设置白名单允许规则。
直接属于 ingress, ingress.from, egress, egress.to 的规则属于或的关系（数组形式），每个规则内的选择则为与的关系（对象形式）。

# Misc.
## [QoS](https://github.com/kubernetes/community/blob/master/contributors/design-proposals/node/pod-resource-management.md)
* Guaranteed: Pod 里每个容器 CPU/Memory 的 request 和 limit 都必须设置且值一样。
* Burstable: Pod 里至少有一个容器设置了 CPU/Memory 的 request。
* BestEffort: Pod 里所有容器都没有设置 CPU/Memory 的 request 和 limit。
（如果设置 limit，而没有设置对应的 request，默认和 limit 一样）
当发生 Pod Eviction 的时候按照 BestEffort, Burstable, Guaranteed 的优先级别进行删除。

cgroup 设置
```
[cpu/memory]/kubepods/[besteffort/burstable]/pod{uid}/containersxxx/
                     /pod{uid}/containersxxx/   # guaranteed
```
cpu limit: cpu.cfs_quota_us, cpu_period_us
cpu request: cpu.shares
memory limit: memory.limit_in_bytes
memory request: 根据`experimental-qos-reserved=memory`参数在 [besteffort/burstable] cgroup 进行预留控制

cpuset
`--cpu-manager-policy` 设为 static（默认none）后，如果 Guaranteed 且是数量是整数，则会使用 cpuset cgroup 绑定到固定的核数上。

## Initializer
1. `InitializerConfiguration` 配置所用 Initializer 的名字及需匹配的资源。
1. 资源的 `metadata.initializers.pending` 中增加 Initializer 名字，并不被创建等待初始化。
1. Initializer (自定义 controller) 根据配置初始化资源，并删除 `metadata.initializers.pending` 对应项。
1. 资源被创建。

## RBAC
* `Role`/`ClusterRole`: 角色，及具体资源的权限规则
* Subject: 作用对象
* `RoleBinding`/`ClusterRoleBinding`: Role 和 Subject 的绑定关系

可以将 RoleBinding 创建在特定 namespace 下，并绑定其他 namespace 的 ServiceAccount 和一个 ClusterRole，以实现只控制特定 namespace 的资源。

Subject:

* `ServiceAccount`, pod 指定后认证信息(对应`kubernetes.io/service-account-token` JWT 类型的Secret)和 apiserver 根证书会挂载在 `/var/run/secrets/kubernetes.io/serviceaccount` 目录。
* `User`: 认证用户名 / `system:serviceaccount:<Namesapce-Name>:<ServiceAccount-Name>` （SA对应）
* `Group`: 认证组名 / `system:serviceaccounts:<Namespace-Name>` （内置，对应Namespace里面的所有SA）

## Monitoring
### metrics
node: 通过 [node exporter](https://github.com/prometheus/node_exporter)
核心组件：通过 `/metrics` API
core metrics: pod, container, node等: 通过 [MetricsServer](https://github.com/kubernetes/community/blob/master/contributors/design-proposals/instrumentation/metrics-server.md) (`/stats/summary` from kubelet cAdvisor)

### ServiceMonitor
1. 应用暴露指标 `http.Handle("/metrics", promhttp.Handler())`
1. Service 暴露端口
1. 创建 `ServiceMonitor`

```
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: myapp
  namespace: default
spec:
  endpoints:
  - interval: 30s
    port: http-metrics
  jobLabel: app
  namespaceSelector:
    matchNames:
    - default
  selector:
    matchLabels:
      app: myapp
```

### MetricsServer
利用 [apiserver aggregation](https://kubernetes.io/docs/tasks/access-kubernetes-api/setup-extension-api-server/)
将 [prometheus adapter](https://github.com/DirectXMan12/k8s-prometheus-adapter) 作为 MetricsServer
```
apiVersion: apiregistration.k8s.io/v1
kind: APIService
metadata:
  name: v1beta1.metrics.k8s.io
spec:
  group: metrics.k8s.io
  groupPriorityMinimum: 100
  insecureSkipTLSVerify: true
  service:
    name: prometheus-adapter
    namespace: monitoring
  version: v1beta1
  versionPriority: 100
```

## Server Side Apply
client usage: https://kubernetes.io/blog/2021/08/06/server-side-apply-ga/

冲突是一种特定的错误状态，发生在执行 Apply 改变一个字段，而恰巧该字段被其他用户声明过主权时 (metadata.managedFields)。
在冲突发生的时候，只有 apply （内容类型为 application/apply-patch+yaml 的 PATCH 请求，且用 field-manager 参数标识自己）操作失败，而 update（所有修改对象的其他操作）会忽略冲突强制覆盖值。

* 覆盖前值，成为唯一的管理器：设置查询参数 force 为 true，这将改变字段的值，从所有其他管理器的 managedFields 条目中删除指定字段。（如果是controller，不代表用户操作，一般会用force）
* 不覆盖前值，放弃管理权：从配置文件中删掉字段，这就保持了原值不变，并从 managedFields 的应用者条目中删除该字段。
* 不覆盖前值，成为共享的管理器：在配置文件中把字段的值改为和服务器对象一样，就实现了字段管理被应用者和所有声明了管理权的其他的字段管理器共享。

apply 时必须字段必须至少有一个 manager (默认字段 manager 为 kube-controller-manager 等系统组件)，不填这个字段相当于放弃自己的管理权。
apply 改变一个字段的管理权时，可以不提供所有必须字段，只提供需要改变的字段，因为其它必须字段已有其它 manager，也不会导致其它 managed 字段被删除。
如果一个 apply 导致一个非必须自段没有 manager，则删除这个字段。只能删除自己 managed 的字段。
如果一个 apply 导致一个必须字段没有 manager，比如放弃自己唯一管理权，则报错。


kubectl (-v7):

cmd | manager | op | method | content-type
-   | -       | -  | -
apply (server) | kubectl | apply | PATCH | apply-patch+yaml
apply | kubectl-client-side-apply | update | PATCH | strategic-merge-patch+json
create | kubectl-create | update | POST | application/json
replace | kubectl-replace | update | PUT | application/json
edit | kubectl-edit | update | PATCH| strategic-merge-patch+json
patch | kubectl-patch | update | PATCH | strategic-merge-patch+json


# Utilities
## Leader Election
[![img](https://cdn-images-1.medium.com/max/800/0*1bwwLXjQrLWL67RQ)](https://medium.com/@dominik.tornow/kubernetes-high-availability-d2c9cbbdd864)

`k8s.io/client-go/tools/leaderelection`

配置：

* `LeaseDuration` 租期
* `RenewDeadline` 续租间隔

`tryAcquireOrRenew` 获得锁的条件（或）：

* 锁资源还没创建，创建并声明获得
* Leader Lease 过期了 （本地当前时间已经超过了最后一次在本地观察到 record 内容变化（即刷新过）的时间+本地配置的租期）
* 现 Leader 就是自己

一般用 `endpoint`/`configmap`/`lease` 表示资源锁对象：
相关信息 `LeaderElectionRecord`（只有`HolderIdentity`被用来判断自己是否是 leader，其他都是记录信息） 存在 `control-plane.alpha.kubernetes.io/leader` annotation 或 lease spec 里面。

```
type LeaderElectionRecord struct {
	HolderIdentity       string      `json:"holderIdentity"`       // hostname + uuid
	LeaseDurationSeconds int         `json:"leaseDurationSeconds"` // 租期
	AcquireTime          metav1.Time `json:"acquireTime"`          // 获取时间
	RenewTime            metav1.Time `json:"renewTime"`            // 刷新时间
	LeaderTransitions    int         `json:"leaderTransitions"`    // 转换次数
}
```

不能保证只有一个 leader (fencing, 如果原 leader 由于网络或系统原因没能及时更新租期，导致 leader 被别人抢到，而自己又没能及时退出，此时2个实列都在运行) , 如果出现2个(split brain)，由 controller 自己靠调谐 (reconciliation) 来解决冲突。
可以容忍绝对时钟偏移值（因为计算都用的本地单调时钟），但会受到时钟偏移率的影响。

# Sigs
## Federation
[![img](https://raw.githubusercontent.com/kubernetes-sigs/kubefed/master/docs/images/concepts.png)](https://github.com/kubernetes-sigs/kubefed)

通过各集群的 API Server（集群配置`KubefedCluster`），在多个集群间同步资源。
kubefed 控制组件安装在其中一个集群（host cluster），其他集群 join `kubefedctl join` 进来。
需创建每个要同步资源的类型配置`kubefedctl enable`(在kubefed ns下创建`FederatedTypeConfig`)。

资源同步: `kubefedctl federate <target kubernetes API type> <target resource> [flags]` (创建对应同步对象`Federated<Kind>`，namespace需先同步)
集群同步的资源始终和同步对象保持一致（更新，删除都会被重置同步回来）。
当同步对象被删除时，集群同步的资源也被删除。
同步对象只在host cluster，所以只能从host cluster向其他集群同步。

资源级别同步配置`Federated<Kind>`：

* Templates: 资源内容表示
* Placement: 需同步的集群
* Overrides: 根据不同集群 Templates 里字段覆盖

# Reference
https://kubernetes.io/docs/home/
https://relnotes.k8s.io/

</xmp>
<script src="js/strapdown.js"></script>
</html>
