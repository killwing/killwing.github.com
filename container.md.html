<!DOCTYPE html>
<html>
<meta charset="utf-8" />
<title>Container</title>
<xmp theme="readable" style="display:none;">
# [namespace](http://man7.org/linux/man-pages/man7/namespaces.7.html)
内核级别全局系统资源隔离。

类型|        定义|             内容
-|-|-
Cgroup      |CLONE_NEWCGROUP   |Cgroup 根目录 (Linux 4.6+)
IPC         |CLONE_NEWIPC      |IPC消息队列
Network     |CLONE_NEWNET      |网络设备，栈，端口等
Mount       |CLONE_NEWNS       |挂载点
PID         |CLONE_NEWPID      |进程ID
User        |CLONE_NEWUSER     |用户ID和组ID
UTS         |CLONE_NEWUTS      |主机名和NIS域名
Time        |CLONE_NEWTIME     |系统时间

`/proc/[pid]/ns/`目录: 如`ipc -> ipc:[4026531839]`
数字是inode号，如果两个进程某个类型的namespace的inode一样，说明同属一个namespace。
当一个namespace中的所有进程都退出时，该namespace将会被删除。

## Network
每个 namespace 都有独立的网络设备，协议栈，IP 地址，路由表，iptables 规则，ipvs。
当 net ns 有进程时 `lsns -t net` 才能列出，且`/run/netns/`下有对应文件，`ip netns list` 可以列出所有创建的 ns。

### 实验
使用虚拟网卡连接（不同 namespace 的）2个容器/进程：
```
host> ip netns add netns1
host> ip netns add netns2
host> ip netns list
host> ip netns exec netns1 /bin/bash --rcfile <(echo "PS1=\"container1> \"")
container1> ip link set dev lo up
host> ip netns exec netns2 /bin/bash --rcfile <(echo "PS1=\"container2> \"")
container2> ip link set dev lo up
host> ip link add veth1 type veth peer name veth2
host> ip link set veth1 netns netns1
host> ip link set veth2 netns netns2
container1> ip addr add 172.17.0.101/24 dev veth1
container1> ip link set dev veth1 up
container2> ip addr add 172.17.0.102/24 dev veth2
container2> ip link set dev veth2 up
```

使用虚拟交换机（网桥）连接多个容器/进程：
```
host> ip netns add netns3
host> ip netns exec netns3 /bin/bash --rcfile <(echo "PS1=\"container3> \"")
container3> ip link set dev lo up
container1> ip link delete veth1 // 只需删除其中一个
host> ip link add br0 type bridge
host> ip link set dev br0 up
host> ip link add veth1 type veth peer name veth1-br
host> ip link add veth2 type veth peer name veth2-br
host> ip link add veth3 type veth peer name veth3-br
host> ip link set dev veth1 netns netns1
host> ip link set dev veth2 netns netns2
host> ip link set dev veth3 netns netns3
host> ip link set dev veth1-br master br0
host> ip link set dev veth2-br master br0
host> ip link set dev veth3-br master br0
host> ip link set dev veth1-br up
host> ip link set dev veth2-br up
host> ip link set dev veth3-br up
container1> ip addr add 172.17.0.101/24 dev veth1
container1> ip link set dev veth1 up
container2> ip addr add 172.17.0.102/24 dev veth2
container2> ip link set dev veth2 up
container3> ip addr add 172.17.0.103/24 dev veth3
container3> ip link set dev veth3 up
```

宿主机访问（不同网段的）容器：Linux 没有提供虚拟路由器设备，但其本身可以充当路由器的功能
```
host> ip addr add 172.17.0.1/24 dev br0 // 如果带上mask，则自动加入目标为 172.17.0.0/24 的 direct link 到路由表
```

容器访问（不同网段的）宿主机：
```
container1> ip route add default via 172.17.0.1
```

容器访问外网/其他主机：在 Linux 中，如果发现收到的 IP 数据包是自己的，则会自己进行接收处理，否则会将其丢弃或者转发。还需启用SNAT，否则对方不识容器地址无法响应(SYN_RECV 状态)。
```
host> cat /proc/sys/net/ipv4/ip_forward
host> sysctl -w net.ipv4.ip_forward=1
host> iptables -t nat -A POSTROUTING -s 172.17.0.0/24 ! -o br0 -j MASQUERADE
host> iptables -t nat -nL
```

外部访问容器：利用 DNAT，把宿主机上指定端口的流量请求转发到容器中的地址 `172.17.0.101:<port>` 中（port publishing/mapping）：
```
host> iptables -t nat -A PREROUTING ! -i br0 -p tcp -m tcp --dport 8000 -j DNAT --to-destination 172.17.0.101:8080
```

内部访问容器（不走 PREROUTING）：
```
host> iptables -t nat -A OUTPUT ! -o br0 -p tcp -m tcp --dport 8000 -j DNAT --to-destination 172.17.0.101:8080
```

使 iptables 能够作用于 bridge 网络：
```
modprobe br_netfilter
sysctl -w net.bridge.bridge-nf-call-arptables=1
sysctl -w net.bridge.bridge-nf-call-iptables=1
```

### 常用命令
`lsns -t net/pid` 列出网络/进程命名空间
`ip netns identify <pid>` 查看进程所属网络命名空间
`ip netns exec <ns-name> <command>` 在网络命名空间下执行命令
`nsenter -a -t <pid> <command>` 进入进程的所有命名空间执行命令，因为mount ns，可能会找不到相应的命令。
`nsenter --net=<file>` file name 同 ns-name，进入网络命名空间，该文件的 inode 即 net ns 的 inode
`unshare --pid --uts --ipc --mount -f chroot rootfs /bin/sh` 在新的 PID, UTS, IPC, and mount namespaces 运行 sh

## User
在一个user ns里，进程可以有完全 root 权限（UID 0），而在 ns 之外（从 parent ns 角度）只有有限权限。
通过 ID 映射，使得一个 user ns 里的进程在 ns 之外有对应 host user 的权限。
当一个新的user ns被创建，进入这个 ns 的**第一个**进程在这个 ns 内具有全部的 capabilities。

`unshare -U [--map-user=<uid>|<name>] [--map-group=<gid>|<name>] [--map-root-user] [--map-current-user]`: 在一个新的 user ns 中将当前用户映射为特定用户和组
`/proc/<pid>/uid_map` 查看 uid 映射，格式 `<mapped-uid> <host-uid> <range>`，range为1表示只映射一个用户

capabilities: 只作用在当前 user ns 下
`capsh --print` 显示此进程 capabilities 状态, e.g. `nsenter --user --preserve-credentials -t <pid> capsh --print`
`getpcaps <pid>` 显示指定进程 capabilities 状态 `cat /proc/<pid>/status | grep Cap`

capability sets:

* Permitted (P): 允许的
* Inheritable (I)：会被子进程继承的
* Effective (E)：有效的
* Bounding：可用的
* Ambient

## Cgroup
使每个容器有独立的 cgroup root 和 cgroup filesytem 视图，隐藏了 host 的 cgroup 信息，更安全；不同 host 间迁移cgroup变得容易。

[CGroup Namespaces](https://lwn.net/Articles/618873)

## syscall
clone：在新namespace中创建一个新的进程
setns：设置当前进程的一个namespace为指定的namespace
unshare：使当前进程退出一个或多个namespace，创建并加入新的namespace
ioctl: 发现 namespace 相关信息

# [cgroup](http://man7.org/linux/man-pages/man7/cgroups.7.html)
对一组进程进行资源的限制和控制。

subsystems(controllers):

[cpu](https://www.kernel.org/doc/Documentation/scheduler/sched-bwc.txt): 限制CPU使用率。
[cpuacct](https://www.kernel.org/doc/Documentation/cgroup-v1/cpuacct.txt): 统计CPU使用率。
[cpuset](https://www.kernel.org/doc/Documentation/cgroup-v1/cpusets.txt): 绑定指定CPU集。
[memory](https://www.kernel.org/doc/Documentation/cgroup-v1/memory.txt): 统计和限制内存使用率。
[devices](https://www.kernel.org/doc/Documentation/cgroup-v1/devices.txt): 限制创建(mknod)和访问设备的权限。
[freezer](https://www.kernel.org/doc/Documentation/cgroup-v1/freezer-subsystem.txt): suspend和restore组中所有进程。例：停止容器的时候需要同步清理所有相关进程，防止逃逸，可以先将其组冻结。
[net_cls](https://www.kernel.org/doc/Documentation/cgroup-v1/net_cls.txt): 创建的所有网络包加上一个classid标记，用于tc（流量控制）和iptables。
[blkio](https://www.kernel.org/doc/Documentation/cgroup-v1/blkio-controller.txt): 限制访问块设备的IO速度。
perf_event: 对cgroup进行性能监控
[net_prio](https://www.kernel.org/doc/Documentation/cgroup-v1/net_prio.txt): 针对每个网络接口设置访问优先级权重。
[hugetlb](https://www.kernel.org/doc/Documentation/cgroup-v1/hugetlb.txt): 限制huge pages的使用量。
[pids](https://www.kernel.org/doc/Documentation/cgroup-v1/pids.txt): 限制总进程数。

查看系统 `/proc/cgroups`
查看进程 `/proc/[pid]/cgroup` 格式：{hierarchy id}:{subsystem}:{相对cgroup root路径}
控制 `/sys/fs/cgroup` subsystem 下建立子目录

 * 子进程 cgroup 和父进程同属一个 cgroup。
 * 进程退出自动从 tasks 里面删除。
 * 进程加入新的 cgroup，自动从原 cgroup 中删除。

主要控制文件
`cpu.cfs_period_us`: 默认 100ms，表示一段 cpu 时间。
`cpu.cfs_quota_us`: 默认 -1，在 `cfs_period_us` 内的最多能占用的时间，如果大于 `cpu.cfs_period_us` 表示占用多于一核。超过时通过 throttling 限制(cpu.stat 查看)。
`cpu.shares`: 设置 cpu 总量（默认 1024），可以让子 cgroup 按比例分配 cpu 资源。
`cpuset.cpus`: 绑定 cpu 核心。
`memory.limit_in_bytes`: 限制内存使用（不算vm，但算 page cache，算在第一次分配 page 的 group 进程上，见下）大小。超过时会回收缓存(page cache)或OOM kill 进程(memory.failcnt 查看)。写入 -1 不限制。
`memory.oom_control`: 默认 0 启用 OOM。
`memory.stat`: 内存统计。
`pid.max`: 总进程（线程）数。
`blkio.throttle.read_bps_device`: 限制块设备读取速度。
`net_cls.classid`: 给报文打标记，需要利用 traffic control 内核模块做流量控制。
`cgroup.clone_children`: 只对cpuset有影响，内容为1时，新创建的cgroup将会继承父cgroup的配置。
`cgroup.procs`: cgroup 中的所有受控制的进程ID。
`cgroup.sane_behavior`
`notify_on_release`: 内容为1时，当cgroup退出时（不再包含任何进程和子cgroup），将调用 `release_agent` 里面配置的命令。
`release_agent`: cgroup退出时将会执行的命令，系统调用该命令时会将相应cgroup的相对路径当作参数传进去。
`tasks`: 线程级别控制，cgroup 中的所有受控制的线程ID。

`/proc` 文件系统并不反映被 cgroup 限制的情况，可以用 [lxcfs](https://github.com/lxc/lxcfs) 解决。

关于内存统计计算：
https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/6/html/resource_management_guide/sec-memory
http://blog.chinaunix.net/uid-20321537-id-3465943.html

[v2](https://man7.org/linux/man-pages/man7/cgroups.7.html#CGROUPS_VERSION_2)
主要控制文件
`cpu.max`
`cpu.weight`
`cpuset.cpus`
`memory.min`
`memory.max`
`memory.high`
`memory.low`
`memory.swap.high`
`memory.swap.low`

`memory.current` = `memory.stat.anno (rss) + memory.stat.file (cache) + memory.stat.kernel + memory.stat.sock`
`memory.stat.file` = `memory.stat.active_file + memory.stat.inactive_file + memory.stat.shmem (tmpfs)`

## tools
cgset
cgget
cgcreate
cgdelete
cgclassify
cgexec

## systemd
systemd 通过 cgroup 来管理进程，根据 cgroup 继承关系可以跟踪服务主进程产生的所有子进程，确保停止服务时清理所有子进程。

service: systemd 服务，daemon 进程
scope: 外部进程，如终端会话及其产生的进程, fork 进程
slice: 用来组织 service 和 scope

systemd-cgtop: 显示资源消耗
systemd-cgls: 列出 cgroup 树结构
`-.slice`: 根
`system.slice`: 系统 service
`user.slice`: 用户会话

临时新建 cgroup: 进程结束，cgroup 释放（和父进程 cgroup 脱离）
`systemd-run --unit=testsvc --slice=test <cmd>` 在 test.slice （默认在system.slice下）下创建 testsvc.service (`--scope` 可以创建 testsvc.scope) cgroup
`systemctl set-property testsvc.service CPUShares=200 MemoryLimit=100M` 设置 cgroup 限制

systemd 服务可在 unit 文件里面限制：
```
[Service]
CPUShares=200
MemoryLimit=100M
```

查看：
systemd-cgls
systemd-cgtop

# rootfs
一个操作系统所包含的文件和目录，但不包含内核。
利用 `chroot`/`pivot_root` 切换进程根目录(`/proc/[pid]/root/`)，达到文件系统隔离的效果。

UnionFS: 将不同目录合并挂载到同一目录下。

## AUFS
[![img](https://docs.docker.com/storage/storagedriver/images/aufs_layers.jpg)](https://docs.docker.com/storage/storagedriver/aufs-driver/)

只读层（多层，镜像层）：ro+wh 挂载，固定不变内容。
init 层：ro+wh 挂载，动态内容，不需要保存到镜像，例 `/etc/hosts`。
读写层（容器层）：rw 挂载，修改：利用 COW 保存修改文件，删除：whiteout，生成 `.wh.foo` 遮挡文件。

## OverlayFS
[![img](https://docs.docker.com/storage/storagedriver/images/overlay_constructs.jpg)](https://docs.docker.com/storage/storagedriver/overlayfs-driver/)

lowerdir: 镜像层
upperdir: 容器层，读写层
workdir: 中间层，写时先写 workdir，再移到 upperdir
mergeddir: 合并层，容器挂载点

读：优先从 upperdir 读
写：copy_up (COW)，复制到 upperdir 写入更新
删：使用 whiteout 文件遮挡
重命名：只允许都在 upperdir 的情况，否则只能复制再删除原来的

性能优越：

* page caching: 多个容器访问（同层）同一个文件，可以共享同一个 page cache。
* copy_up: 相比 AUFS 都是采用了 COW 机制，但 AUFS 需要在多层里面搜索文件，而 OverlayFS 只有两层，且可以利用缓存。
* inode limits: overlay2 解决了 overlay 的 inode 消耗问题。

overlay2 和 overlay 的主要区别就是为了在单层 lowerdir 中支持多层镜像而采用了不同方式:

* overlay: overlayfs lowerdir 只支持单层。使用硬链接的方式，在 lowerdir 层中共享不同镜像层的文件。由于每层都会建立对底层文件的硬链接，这些链接文件所在的大量的新目录会（目录不能做硬链接）消耗大量 inode 数量。
* overlay2: overlayfs (kernel 4.0+) 直接支持 lowerdir 用`:`隔开使用多层目录，但最多 128 层。

docker:
`/var/lib/docker/overlay2/l`: 使用缩短的 ID 层软链接，避免 mount 命令参数超出页大小限制。

每层内容

* link 文件：包含缩短ID
* diff 目录：实际内容
* lower 文件：只有上层有，所有本层的下层的缩短ID列表
* merged 目录：合并 upperdir 和 lowerdir 的内容
* work 目录：overlayfs 内部使用，如实现 COW

例子(`docker run ubuntu:16.04`)：
```
// docker image inspect
"GraphDriver": {
    "Data": {
        "LowerDir": "/var/lib/docker/overlay2/07658f200ae0d7953ba35aaf7327ced46fa174ae9c53c3e9a393d0a5386f1d64/diff:     # 底下 4 层
                     /var/lib/docker/overlay2/527c42d27c58740ad41d4ec02ec7b4a92ab477140e7f4bbf9ed430b08d85658b/diff:
                     /var/lib/docker/overlay2/7245916f11daaca5bf0dd59660b664a4d62a62a748f868e97fd8b74c0dbf459c/diff:
                     /var/lib/docker/overlay2/f3b6eec681496d4a8b3e8a7dfddc2918cabcf7b01b97260dbd0016c90bc62cfa/diff",
        "MergedDir": "/var/lib/docker/overlay2/210f88b50e92e5000061e33cc7e8a1af47b1d3f0323fefcf4b7844a13680c2aa/merged", # 合并层
        "UpperDir": "/var/lib/docker/overlay2/210f88b50e92e5000061e33cc7e8a1af47b1d3f0323fefcf4b7844a13680c2aa/diff",    # 顶层
        "WorkDir": "/var/lib/docker/overlay2/210f88b50e92e5000061e33cc7e8a1af47b1d3f0323fefcf4b7844a13680c2aa/work"
    },
    "Name": "overlay2"
},
"RootFS": { # 共 5 层
    "Type": "layers",
    "Layers": [
        "sha256:8823818c474862932702f8a920abea43b2560ddceb910d145be9ba0eb149a643",
        "sha256:4a7a5ec0f29e4172be88650833b9e6ba9e3dd6c231359357a778bdc6bfbbe86f",
        "sha256:87a2d000062258f5b4628561771fa14780688808cf6df7cb93f948695b96e1f8",
        "sha256:07663827a77f781d46aaeb1b1492484f45970d1cdd5b9e1e4e35680ce15d41d9",
        "sha256:d7232280c8c479f251dffc9baf4eb025b8d372820678fac87fe0c4dde00ca9cb"
    ]
},

// mount info, mount -t overlay overlay -o lowerdir:/lower1:/lower2:/lower3:...,upperdir=/upper,workdir=/work /merged
overlay on /var/lib/docker/overlay2/772385039e04b1490772892bec516f8878dcc46da79c012620e316c49c4de672/merged type overlay
(rw,relatime,
lowerdir=/var/lib/docker/overlay2/l/IBHM6VAXQLYIL5UE62M6SLJSMX:
         /var/lib/docker/overlay2/l/XLR3UQOJ56H6N5NEU5XHYOPE56:
         /var/lib/docker/overlay2/l/NJJWYRTW3ALLCTMXXMWJRJAIGW:
         /var/lib/docker/overlay2/l/U4BLJW56SOJMZ5XKVUHTMKHBG3:
         /var/lib/docker/overlay2/l/5ZOHJPJHUMRBSFTAZZQUUZCVOU:
         /var/lib/docker/overlay2/l/P4RWIRDVRNUFKOGVPIVZAW2DP5,
upperdir=/var/lib/docker/overlay2/772385039e04b1490772892bec516f8878dcc46da79c012620e316c49c4de672/diff,
workdir=/var/lib/docker/overlay2/772385039e04b1490772892bec516f8878dcc46da79c012620e316c49c4de672/work)

// docker container inspect
"GraphDriver": {
    "Data": {
        "LowerDir": "/var/lib/docker/overlay2/772385039e04b1490772892bec516f8878dcc46da79c012620e316c49c4de672-init/diff:  # init 层
                     /var/lib/docker/overlay2/210f88b50e92e5000061e33cc7e8a1af47b1d3f0323fefcf4b7844a13680c2aa/diff:       # 镜像顶层
                     /var/lib/docker/overlay2/07658f200ae0d7953ba35aaf7327ced46fa174ae9c53c3e9a393d0a5386f1d64/diff:       # 镜像下层
                     /var/lib/docker/overlay2/527c42d27c58740ad41d4ec02ec7b4a92ab477140e7f4bbf9ed430b08d85658b/diff:
                     /var/lib/docker/overlay2/7245916f11daaca5bf0dd59660b664a4d62a62a748f868e97fd8b74c0dbf459c/diff:
                     /var/lib/docker/overlay2/f3b6eec681496d4a8b3e8a7dfddc2918cabcf7b01b97260dbd0016c90bc62cfa/diff",
        "MergedDir": "/var/lib/docker/overlay2/772385039e04b1490772892bec516f8878dcc46da79c012620e316c49c4de672/merged",   # 合并层
        "UpperDir": "/var/lib/docker/overlay2/772385039e04b1490772892bec516f8878dcc46da79c012620e316c49c4de672/diff",      # 容器层
        "WorkDir": "/var/lib/docker/overlay2/772385039e04b1490772892bec516f8878dcc46da79c012620e316c49c4de672/work"
    },
    "Name": "overlay2"
},
```

镜像分析工具 [dive](https://github.com/wagoodman/dive)

## DeviceMapper
块设备

# Reference
http://lwn.net/Articles/531114/
https://segmentfault.com/a/1190000009732550

</xmp>
<script src="js/strapdown.js"></script>
</html>
